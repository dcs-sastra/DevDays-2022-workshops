{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-07T16:55:53.03711Z","iopub.status.busy":"2022-05-07T16:55:53.036842Z","iopub.status.idle":"2022-05-07T16:56:01.92132Z","shell.execute_reply":"2022-05-07T16:56:01.920376Z","shell.execute_reply.started":"2022-05-07T16:55:53.037082Z"},"trusted":true},"outputs":[],"source":["%pip install contractions\n","\n","# ------------------------------------------------------------------------------------------- #\n","\n","import pandas as pd\n","import numpy as np\n","import contractions\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords, wordnet\n","\n","# ------------------------------------------------------------------------------------------- #\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","\n","# ------------------------------------------------------------------------------------------- #\n","\n","from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, LSTM, Bidirectional\n","from tensorflow.keras.models import Sequential\n","\n","# ------------------------------------------------------------------------------------------- #\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# ------------------------------------------------------------------------------------------- #\n","\n","import matplotlib.pyplot as plt\n","\n","# ------------------------------------------------------------------------------------------- #\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:33:13.909165Z","iopub.status.busy":"2022-05-07T16:33:13.908928Z","iopub.status.idle":"2022-05-07T16:33:14.026119Z","shell.execute_reply":"2022-05-07T16:33:14.025354Z","shell.execute_reply.started":"2022-05-07T16:33:13.909134Z"},"trusted":true},"outputs":[],"source":["train_data = pd.____(\"____\", delimiter = \";\", names = [\"text\", \"sentiment\"])\n","test_data = pd.read_csv(\"____\", delimiter = \";\", names = [\"text\", \"sentiment\"])\n","display(train_data.head())\n","display(test_data.info())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:33:14.028036Z","iopub.status.busy":"2022-05-07T16:33:14.027594Z","iopub.status.idle":"2022-05-07T16:33:14.042647Z","shell.execute_reply":"2022-05-07T16:33:14.041896Z","shell.execute_reply.started":"2022-05-07T16:33:14.027997Z"},"trusted":true},"outputs":[],"source":["def expand_contractions(df_series):\n","    \"\"\" Expands contractions from text in pandas series.\n","        (Eg: can't --> cannot)\n","        \n","    Args:\n","        df_series (pd.Series): Pandas series containing text data.\n","    \n","    Returns:\n","        df_series (pd.Series): Pandas series containing text data after \n","                               expanding contractions.\n","    \"\"\"\n","    \n","    for i in range(len(df_series)):\n","        df_series[i] = contractions.fix(df_series[i])\n","    \n","    return df_series\n","\n","\n","def get_pos(token):\n","    \"\"\" Returns \"part of speech\" of the token which is understandable \n","        by WordNetLemmatizer.\n","        \n","    Args:\n","        token (str): Single token whose POS to be identified.\n","    \n","    Returns:\n","        (str): POS tag of the token in a format understandable by WordNetLemmatizer.\n","    \"\"\"\n","    \n","    pos_tag = nltk.pos_tag(token)[0][1][0].upper()\n","    pos_tag_dict = {\"J\": wordnet.ADJ,\n","                    \"N\": wordnet.NOUN,\n","                    \"V\": wordnet.VERB,\n","                    \"R\": wordnet.ADV}\n","    \n","    # Returns wordnet.NOUN as default if it can't find exact POS \n","    return pos_tag_dict.get(pos_tag, wordnet.NOUN)\n","\n","\n","def lemmatize_series(df_series, remove_stopwords=False):\n","    \"\"\" Lemmatizes text data in pandas series and removes stopwords.\n","        \n","    Args:\n","        df_series (pd.Series): Pandas series containing text data.\n","        remove_stopwords (bool): Removes stopwords from the text if True. \n","                                 Defaults to False.\n","                                 \n","    Returns:\n","        df_series (pd.Series): Pandas series containing lemmatized text data \n","                               without stopwords if specified.\n","    \"\"\"\n","    \n","    if remove_stopwords:\n","        stop_words = set(stopwords.words(\"english\"))\n","        lm = WordNetLemmatizer()\n","        for i in range(len(df_series)):\n","            df_series[i] = ' '.join(\n","                [\n","                    lm.lemmatize(word, get_pos(word)) \n","                    for word in df_series[i].split() \n","                    if not word.lower() in stop_words\n","                ]\n","            )\n","    else:\n","        lm = WordNetLemmatizer()\n","        for i in range(len(df_series)):\n","            df_series[i] = ' '.join(\n","                [\n","                    lm.lemmatize(word, get_pos(word)) \n","                    for word in df_series[i].split()\n","                ]\n","            )\n","    \n","    return df_series\n","\n","\n","def preprocess_text(df_series, remove_stopwords=True):\n","    \"\"\" Removes all non-alphanumeric characters except whitespace.\n","        \n","    Args:\n","        df_series (pd.series): Pandas series object containing text.\n","        remove_stopwords (bool): Removes stopwords from text if True. Defaults to True. \n","        \n","    Returns:\n","        df_series (pd.series): Pandas series object containing preprocessed text. \n","    \"\"\"\n","    \n","    # Expand contractions (Eg: can't --> cannot)\n","    df_series = expand_contractions(df_series)\n","    \n","    # Removes non alphanumeric characters\n","    df_series = df_series.str.replace(\"[^a-zA-Z0-9 ]\", \" \")\n","    \n","    # Lemmatize text\n","    df_series = lemmatize_series(df_series, remove_stopwords = remove_stopwords)\n","    \n","    return df_series"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:33:14.045142Z","iopub.status.busy":"2022-05-07T16:33:14.04486Z","iopub.status.idle":"2022-05-07T16:34:40.903863Z","shell.execute_reply":"2022-05-07T16:34:40.903161Z","shell.execute_reply.started":"2022-05-07T16:33:14.045101Z"},"trusted":true},"outputs":[],"source":["train_data[\"text\"] = preprocess_text(train_data[\"text\"])\n","test_data[\"text\"] = preprocess_text(test_data[\"text\"])\n","display(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:40.905663Z","iopub.status.busy":"2022-05-07T16:34:40.905397Z","iopub.status.idle":"2022-05-07T16:34:41.158842Z","shell.execute_reply":"2022-05-07T16:34:41.158138Z","shell.execute_reply.started":"2022-05-07T16:34:40.905627Z"},"trusted":true},"outputs":[],"source":["# Plotting boxplot for number of tokens in each observation\n","ax = train_data[\"text\"].str.split().map(lambda x: len(x)).plot.box(figsize=(6,8))\n","ax.set_ylabel(\"( Number of Tokens )\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:41.160649Z","iopub.status.busy":"2022-05-07T16:34:41.160118Z","iopub.status.idle":"2022-05-07T16:34:41.172091Z","shell.execute_reply":"2022-05-07T16:34:41.171352Z","shell.execute_reply.started":"2022-05-07T16:34:41.160606Z"},"trusted":true},"outputs":[],"source":["# Preprocessing targets\n","le = LabelEncoder()\n","le.fit(train_data[\"sentiment\"])\n","\n","train_targets = le.____(train_data[\"sentiment\"])\n","train_targets = to_categorical(np.asarray(train_targets))\n","\n","test_targets = le.transform(test_data[\"sentiment\"])\n","test_targets = ____(np.asarray(test_targets))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:41.174349Z","iopub.status.busy":"2022-05-07T16:34:41.173669Z","iopub.status.idle":"2022-05-07T16:34:41.181817Z","shell.execute_reply":"2022-05-07T16:34:41.18103Z","shell.execute_reply.started":"2022-05-07T16:34:41.174309Z"},"trusted":true},"outputs":[],"source":["# Defining parameters\n","vocab_size = 10000\n","embedding_dim = 32\n","max_len = 25\n","trunc_type = \"____\"\n","padding_type = \"____\"\n","oov_token = \"<OOV>\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:41.185169Z","iopub.status.busy":"2022-05-07T16:34:41.18303Z","iopub.status.idle":"2022-05-07T16:34:41.433093Z","shell.execute_reply":"2022-05-07T16:34:41.432337Z","shell.execute_reply.started":"2022-05-07T16:34:41.185135Z"},"trusted":true},"outputs":[],"source":["# Fitting tokenizer\n","tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)\n","tokenizer.fit_on_texts(train_data[\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:41.434545Z","iopub.status.busy":"2022-05-07T16:34:41.434269Z","iopub.status.idle":"2022-05-07T16:34:41.743044Z","shell.execute_reply":"2022-05-07T16:34:41.742262Z","shell.execute_reply.started":"2022-05-07T16:34:41.43451Z"},"trusted":true},"outputs":[],"source":["# Converting texts to sequences and padding them to make\n","# them compatible with embedding layers\n","\n","training_seq = tokenizer.texts_to_sequences(train_data[\"text\"])\n","training_padded = pad_sequences(\n","    training_seq,\n","    truncating = trunc_type,\n","    padding = padding_type,\n","    maxlen = max_len\n",")\n","\n","test_seq = tokenizer.____(test_data[\"text\"])\n","test_padded = ____(\n","    test_seq,\n","    truncating = trunc_type,\n","    padding = padding_type,\n","    maxlen = max_len\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:41.7459Z","iopub.status.busy":"2022-05-07T16:34:41.745698Z","iopub.status.idle":"2022-05-07T16:34:41.756846Z","shell.execute_reply":"2022-05-07T16:34:41.755798Z","shell.execute_reply.started":"2022-05-07T16:34:41.745875Z"},"trusted":true},"outputs":[],"source":["# Label Encoding targets\n","train_data[\"sentiment\"].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:41.760459Z","iopub.status.busy":"2022-05-07T16:34:41.757973Z","iopub.status.idle":"2022-05-07T16:34:44.280554Z","shell.execute_reply":"2022-05-07T16:34:44.279705Z","shell.execute_reply.started":"2022-05-07T16:34:41.760414Z"},"trusted":true},"outputs":[],"source":["lstm_model = Sequential([\n","        Embedding(vocab_size, ____, input_length = ____),\n","        Bidirectional(LSTM(64)),\n","        Dense(256, activation = '____'),\n","        Dense(6, activation = '____')\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:44.28232Z","iopub.status.busy":"2022-05-07T16:34:44.282003Z","iopub.status.idle":"2022-05-07T16:34:44.30374Z","shell.execute_reply":"2022-05-07T16:34:44.30289Z","shell.execute_reply.started":"2022-05-07T16:34:44.282262Z"},"trusted":true},"outputs":[],"source":["lstm_model.compile(loss = \"____\", optimizer = \"____\", metrics = [\"____\"])\n","lstm_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:44.305784Z","iopub.status.busy":"2022-05-07T16:34:44.305018Z","iopub.status.idle":"2022-05-07T16:34:44.310101Z","shell.execute_reply":"2022-05-07T16:34:44.309305Z","shell.execute_reply.started":"2022-05-07T16:34:44.305735Z"},"trusted":true},"outputs":[],"source":["NUM_EPOCHS = 10\n","BATCH_SIZE = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:44.311976Z","iopub.status.busy":"2022-05-07T16:34:44.311531Z","iopub.status.idle":"2022-05-07T16:34:44.321084Z","shell.execute_reply":"2022-05-07T16:34:44.320244Z","shell.execute_reply.started":"2022-05-07T16:34:44.311934Z"},"trusted":true},"outputs":[],"source":["es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 100, patience = 3)\n","mc = ModelCheckpoint(\n","    filepath = \"./checkpoint\",\n","    monitor = 'val_accuracy',\n","    mode = 'max',\n","    save_best_only = True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:34:44.324665Z","iopub.status.busy":"2022-05-07T16:34:44.324412Z","iopub.status.idle":"2022-05-07T16:35:49.733593Z","shell.execute_reply":"2022-05-07T16:35:49.732492Z","shell.execute_reply.started":"2022-05-07T16:34:44.324625Z"},"trusted":true},"outputs":[],"source":["history = lstm_model.fit(\n","    training_padded,\n","    train_targets,\n","    validation_data = (test_padded, test_targets),\n","    epochs = NUM_EPOCHS,\n","    batch_size = BATCH_SIZE,\n","    callbacks = [es, mc]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:35:49.735506Z","iopub.status.busy":"2022-05-07T16:35:49.735236Z","iopub.status.idle":"2022-05-07T16:35:49.928874Z","shell.execute_reply":"2022-05-07T16:35:49.928134Z","shell.execute_reply.started":"2022-05-07T16:35:49.735471Z"},"trusted":true},"outputs":[],"source":["history_dict = history.history\n","train_acc = history_dict['loss']\n","val_acc = history_dict['val_loss']\n","epochs = range(1, len(history_dict['loss'])+1)\n","plt.plot(epochs, train_acc,'b', label='Training error')\n","plt.plot(epochs, val_acc,'b', color=\"orange\", label='Validation error')\n","plt.title('Training and Validation error')\n","plt.xlabel('Epochs')\n","plt.ylabel('Error')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:56:37.787864Z","iopub.status.busy":"2022-05-07T16:56:37.786949Z","iopub.status.idle":"2022-05-07T16:56:38.087185Z","shell.execute_reply":"2022-05-07T16:56:38.086424Z","shell.execute_reply.started":"2022-05-07T16:56:37.787815Z"},"trusted":true},"outputs":[],"source":["y_preds = lstm_model.____(test_padded)\n","y_preds = np.argmax(y_preds, axis = 1)\n","\n","test_targets = np.argmax(test_targets, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:56:53.851916Z","iopub.status.busy":"2022-05-07T16:56:53.851658Z","iopub.status.idle":"2022-05-07T16:56:54.165648Z","shell.execute_reply":"2022-05-07T16:56:54.164361Z","shell.execute_reply.started":"2022-05-07T16:56:53.851888Z"},"trusted":true},"outputs":[],"source":["cm = confusion_matrix(test_targets, y_preds)\n","\n","disp = ConfusionMatrixDisplay(\n","    confusion_matrix = cm,\n","    display_labels = le.classes_\n",")\n","disp.plot()\n","plt.show()\n","\n","print(classification_report(test_targets, y_preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-07T16:35:50.6538Z","iopub.status.busy":"2022-05-07T16:35:50.653529Z","iopub.status.idle":"2022-05-07T16:36:01.035219Z","shell.execute_reply":"2022-05-07T16:36:01.034323Z","shell.execute_reply.started":"2022-05-07T16:35:50.653766Z"},"trusted":true},"outputs":[],"source":["lstm_model.____(\"./model\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
