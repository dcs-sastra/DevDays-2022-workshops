{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install contractions\n\n# ------------------------------------------------------------------------------------------- #\n\nimport pandas as pd\nimport numpy as np\nimport contractions\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords, wordnet\n\n# ------------------------------------------------------------------------------------------- #\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\n# ------------------------------------------------------------------------------------------- #\n\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Flatten, LSTM, Bidirectional\nfrom tensorflow.keras.models import Sequential\n\n# ------------------------------------------------------------------------------------------- #\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# ------------------------------------------------------------------------------------------- #\n\nimport matplotlib.pyplot as plt\n\n# ------------------------------------------------------------------------------------------- #\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T17:32:05.780582Z","iopub.execute_input":"2022-05-07T17:32:05.78139Z","iopub.status.idle":"2022-05-07T17:32:23.915592Z","shell.execute_reply.started":"2022-05-07T17:32:05.781288Z","shell.execute_reply":"2022-05-07T17:32:23.914818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/dcsdlworkshop/NLP data/train.txt\", delimiter = \";\", names = [\"text\", \"sentiment\"])\ntest_data = pd.read_csv(\"../input/dcsdlworkshop/NLP data/test.txt\", delimiter = \";\", names = [\"text\", \"sentiment\"])\ndisplay(train_data.head())\ndisplay(test_data.info())","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:32:23.917115Z","iopub.execute_input":"2022-05-07T17:32:23.917355Z","iopub.status.idle":"2022-05-07T17:32:24.011539Z","shell.execute_reply.started":"2022-05-07T17:32:23.917307Z","shell.execute_reply":"2022-05-07T17:32:24.010766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def expand_contractions(df_series):\n    \"\"\" Expands contractions from text in pandas series.\n        (Eg: can't --> cannot)\n        \n    Args:\n        df_series (pd.Series): Pandas series containing text data.\n    \n    Returns:\n        df_series (pd.Series): Pandas series containing text data after \n                               expanding contractions.\n    \"\"\"\n    \n    for i in range(len(df_series)):\n        df_series[i] = contractions.fix(df_series[i])\n    \n    return df_series\n\n\ndef get_pos(token):\n    \"\"\" Returns \"part of speech\" of the token which is understandable \n        by WordNetLemmatizer.\n        \n    Args:\n        token (str): Single token whose POS to be identified.\n    \n    Returns:\n        (str): POS tag of the token in a format understandable by WordNetLemmatizer.\n    \"\"\"\n    \n    pos_tag = nltk.pos_tag(token)[0][1][0].upper()\n    pos_tag_dict = {\"J\": wordnet.ADJ,\n                    \"N\": wordnet.NOUN,\n                    \"V\": wordnet.VERB,\n                    \"R\": wordnet.ADV}\n    \n    # Returns wordnet.NOUN as default if it can't find exact POS \n    return pos_tag_dict.get(pos_tag, wordnet.NOUN)\n\n\ndef lemmatize_series(df_series, remove_stopwords=False):\n    \"\"\" Lemmatizes text data in pandas series and removes stopwords.\n        \n    Args:\n        df_series (pd.Series): Pandas series containing text data.\n        remove_stopwords (bool): Removes stopwords from the text if True. \n                                 Defaults to False.\n                                 \n    Returns:\n        df_series (pd.Series): Pandas series containing lemmatized text data \n                               without stopwords if specified.\n    \"\"\"\n    \n    if remove_stopwords:\n        stop_words = set(stopwords.words(\"english\"))\n        lm = WordNetLemmatizer()\n        for i in range(len(df_series)):\n            df_series[i] = ' '.join(\n                [\n                    lm.lemmatize(word, get_pos(word)) \n                    for word in df_series[i].split() \n                    if not word.lower() in stop_words\n                ]\n            )\n    else:\n        lm = WordNetLemmatizer()\n        for i in range(len(df_series)):\n            df_series[i] = ' '.join(\n                [\n                    lm.lemmatize(word, get_pos(word)) \n                    for word in df_series[i].split()\n                ]\n            )\n    \n    return df_series\n\n\ndef preprocess_text(df_series, remove_stopwords=True):\n    \"\"\" Removes all non-alphanumeric characters except whitespace.\n        \n    Args:\n        df_series (pd.series): Pandas series object containing text.\n        remove_stopwords (bool): Removes stopwords from text if True. Defaults to True. \n        \n    Returns:\n        df_series (pd.series): Pandas series object containing preprocessed text. \n    \"\"\"\n    \n    # Expand contractions (Eg: can't --> cannot)\n    df_series = expand_contractions(df_series)\n    \n    # Removes non alphanumeric characters\n    df_series = df_series.str.replace(\"[^a-zA-Z0-9 ]\", \" \")\n    \n    # Lemmatize text\n    df_series = lemmatize_series(df_series, remove_stopwords = remove_stopwords)\n    \n    return df_series","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:32:24.013069Z","iopub.execute_input":"2022-05-07T17:32:24.013551Z","iopub.status.idle":"2022-05-07T17:32:24.027469Z","shell.execute_reply.started":"2022-05-07T17:32:24.013514Z","shell.execute_reply":"2022-05-07T17:32:24.026766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"text\"] = preprocess_text(train_data[\"text\"])\ntest_data[\"text\"] = preprocess_text(test_data[\"text\"])\ndisplay(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:32:24.029971Z","iopub.execute_input":"2022-05-07T17:32:24.030268Z","iopub.status.idle":"2022-05-07T17:33:50.642249Z","shell.execute_reply.started":"2022-05-07T17:32:24.030206Z","shell.execute_reply":"2022-05-07T17:33:50.641579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting boxplot for number of tokens in each observation\nax = train_data[\"text\"].str.split().map(lambda x: len(x)).plot.box(figsize=(6,8))\nax.set_ylabel(\"( Number of Tokens )\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:50.64358Z","iopub.execute_input":"2022-05-07T17:33:50.643821Z","iopub.status.idle":"2022-05-07T17:33:50.89033Z","shell.execute_reply.started":"2022-05-07T17:33:50.643788Z","shell.execute_reply":"2022-05-07T17:33:50.889675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing targets\nle = LabelEncoder()\nle.fit(train_data[\"sentiment\"])\n\ntrain_targets = le.transform(train_data[\"sentiment\"])\ntrain_targets = to_categorical(np.asarray(train_targets))\n\ntest_targets = le.transform(test_data[\"sentiment\"])\ntest_targets = to_categorical(np.asarray(test_targets))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:50.891608Z","iopub.execute_input":"2022-05-07T17:33:50.892008Z","iopub.status.idle":"2022-05-07T17:33:50.902777Z","shell.execute_reply.started":"2022-05-07T17:33:50.891972Z","shell.execute_reply":"2022-05-07T17:33:50.902109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining parameters\nvocab_size = 10000\nembedding_dim = 32\nmax_len = 25\ntrunc_type = \"post\"\npadding_type = \"post\"\noov_token = \"<OOV>\"","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:50.904007Z","iopub.execute_input":"2022-05-07T17:33:50.904309Z","iopub.status.idle":"2022-05-07T17:33:50.912184Z","shell.execute_reply.started":"2022-05-07T17:33:50.904274Z","shell.execute_reply":"2022-05-07T17:33:50.911451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting tokenizer\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)\ntokenizer.fit_on_texts(train_data[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:50.913418Z","iopub.execute_input":"2022-05-07T17:33:50.913698Z","iopub.status.idle":"2022-05-07T17:33:51.151193Z","shell.execute_reply.started":"2022-05-07T17:33:50.913644Z","shell.execute_reply":"2022-05-07T17:33:51.150486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting texts to sequences and padding them to make\n# them compatible with embedding layers\n\ntraining_seq = tokenizer.texts_to_sequences(train_data[\"text\"])\ntraining_padded = pad_sequences(\n    training_seq,\n    truncating = trunc_type,\n    padding = padding_type,\n    maxlen = max_len\n)\n\ntest_seq = tokenizer.texts_to_sequences(test_data[\"text\"])\ntest_padded = pad_sequences(\n    test_seq,\n    truncating = trunc_type,\n    padding = padding_type,\n    maxlen = max_len\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:51.152413Z","iopub.execute_input":"2022-05-07T17:33:51.152677Z","iopub.status.idle":"2022-05-07T17:33:51.434447Z","shell.execute_reply.started":"2022-05-07T17:33:51.152642Z","shell.execute_reply":"2022-05-07T17:33:51.433698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding targets\ntrain_data[\"sentiment\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:51.436755Z","iopub.execute_input":"2022-05-07T17:33:51.436954Z","iopub.status.idle":"2022-05-07T17:33:51.443686Z","shell.execute_reply.started":"2022-05-07T17:33:51.436929Z","shell.execute_reply":"2022-05-07T17:33:51.442965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model = Sequential([\n        Embedding(vocab_size, embedding_dim, input_length = max_len),\n        Bidirectional(LSTM(64)),\n        Dense(256, activation = 'relu'),\n        Dense(6, activation = 'softmax')\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:51.444692Z","iopub.execute_input":"2022-05-07T17:33:51.445161Z","iopub.status.idle":"2022-05-07T17:33:54.327412Z","shell.execute_reply.started":"2022-05-07T17:33:51.445125Z","shell.execute_reply":"2022-05-07T17:33:54.326697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\nlstm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:54.328497Z","iopub.execute_input":"2022-05-07T17:33:54.328747Z","iopub.status.idle":"2022-05-07T17:33:54.346808Z","shell.execute_reply.started":"2022-05-07T17:33:54.328712Z","shell.execute_reply":"2022-05-07T17:33:54.34611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 10\nBATCH_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:54.348085Z","iopub.execute_input":"2022-05-07T17:33:54.348573Z","iopub.status.idle":"2022-05-07T17:33:54.352144Z","shell.execute_reply.started":"2022-05-07T17:33:54.348535Z","shell.execute_reply":"2022-05-07T17:33:54.351455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 100, patience = 3)\nmc = ModelCheckpoint(\n    filepath = \"./checkpoint\",\n    monitor = 'val_accuracy',\n    mode = 'max',\n    save_best_only = True\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:54.353336Z","iopub.execute_input":"2022-05-07T17:33:54.353772Z","iopub.status.idle":"2022-05-07T17:33:54.360935Z","shell.execute_reply.started":"2022-05-07T17:33:54.353735Z","shell.execute_reply":"2022-05-07T17:33:54.360174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = lstm_model.fit(\n    training_padded,\n    train_targets,\n    validation_data = (test_padded, test_targets),\n    epochs = NUM_EPOCHS,\n    batch_size = BATCH_SIZE,\n    callbacks = [es, mc]\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:33:54.362114Z","iopub.execute_input":"2022-05-07T17:33:54.362415Z","iopub.status.idle":"2022-05-07T17:35:11.106943Z","shell.execute_reply.started":"2022-05-07T17:33:54.36238Z","shell.execute_reply":"2022-05-07T17:35:11.105899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history.history\ntrain_acc = history_dict['loss']\nval_acc = history_dict['val_loss']\nepochs = range(1, len(history_dict['loss'])+1)\nplt.plot(epochs, train_acc,'b', label='Training error')\nplt.plot(epochs, val_acc,'b', color=\"orange\", label='Validation error')\nplt.title('Training and Validation error')\nplt.xlabel('Epochs')\nplt.ylabel('Error')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:35:11.108897Z","iopub.execute_input":"2022-05-07T17:35:11.109464Z","iopub.status.idle":"2022-05-07T17:35:18.735854Z","shell.execute_reply.started":"2022-05-07T17:35:11.109409Z","shell.execute_reply":"2022-05-07T17:35:18.735128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = lstm_model.predict(test_padded)\ny_preds = np.argmax(y_preds, axis = 1)\n\ntest_targets = np.argmax(test_targets, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:35:18.737129Z","iopub.execute_input":"2022-05-07T17:35:18.737394Z","iopub.status.idle":"2022-05-07T17:35:19.425903Z","shell.execute_reply.started":"2022-05-07T17:35:18.737349Z","shell.execute_reply":"2022-05-07T17:35:19.42514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(test_targets, y_preds)\n\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix = cm,\n    display_labels = le.classes_\n)\ndisp.plot()\nplt.show()\n\nprint(classification_report(test_targets, y_preds))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:35:19.427229Z","iopub.execute_input":"2022-05-07T17:35:19.427495Z","iopub.status.idle":"2022-05-07T17:35:19.72924Z","shell.execute_reply.started":"2022-05-07T17:35:19.42746Z","shell.execute_reply":"2022-05-07T17:35:19.728507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model.save(\"./model\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:35:19.730389Z","iopub.execute_input":"2022-05-07T17:35:19.731298Z","iopub.status.idle":"2022-05-07T17:35:29.806835Z","shell.execute_reply.started":"2022-05-07T17:35:19.731254Z","shell.execute_reply":"2022-05-07T17:35:29.806079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}